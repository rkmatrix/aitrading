# configs/phase76_train_execution_aware.yaml

env:
  id: "TradingEnv-v0"         # will try to use if available, else dummy env
  execution_reward:
    slippage_weight: -1.0
    spread_weight: -0.5
    latency_weight: -0.0005
    fill_prob_weight: 1.0
    clamp_penalty: -0.3
    pred_block_penalty: -2.0
    risk_block_penalty: -3.0

train:
  total_timesteps: 100000     # adjust higher when things are stable
  policy: "MlpPolicy"
  gamma: 0.99
  learning_rate: 3.0e-4
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  ent_coef: 0.0
  clip_range: 0.2

output:
  policy_name: "EquityRLPolicyExecAware"
  base_dir: "models/policies"
  log_dir: "logs/phase76"
